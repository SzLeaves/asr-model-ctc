{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CTC + BiGRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:34:59.855505Z",
     "iopub.status.busy": "2023-02-21T13:34:59.855283Z",
     "iopub.status.idle": "2023-02-21T13:35:02.207295Z",
     "shell.execute_reply": "2023-02-21T13:35:02.206567Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "keras = tf.keras\n",
    "ctc_batch_cost = keras.backend.ctc_batch_cost\n",
    "Model = keras.models.Model\n",
    "Adam = keras.optimizers.Adam\n",
    "Input, Dense, GRU, Lambda, add = (\n",
    "    keras.layers.Input,\n",
    "    keras.layers.Dense,\n",
    "    keras.layers.GRU,\n",
    "    keras.layers.Lambda,\n",
    "    keras.layers.add,\n",
    ")\n",
    "\n",
    "# 设置显存大小\n",
    "# gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# memory_size = tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7550)\n",
    "# tf.config.experimental.set_virtual_device_configuration(gpus[0], [memory_size])\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Microsoft YaHei\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# 音频/语音标注文件路径\n",
    "DS_PATH = \"data/\"\n",
    "# 模型文件路径\n",
    "MODELS_PATH = \"model/\"\n",
    "# mfcc特征维数\n",
    "MFCC_VALUE = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.210861Z",
     "iopub.status.busy": "2023-02-21T13:35:02.210133Z",
     "iopub.status.idle": "2023-02-21T13:35:02.423958Z",
     "shell.execute_reply": "2023-02-21T13:35:02.422989Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取音频特征\n",
    "with open(MODELS_PATH + 'data_mfcc.pkl', 'rb') as file:\n",
    "    train_ds, mfcc_mean, mfcc_std = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.427766Z",
     "iopub.status.busy": "2023-02-21T13:35:02.426982Z",
     "iopub.status.idle": "2023-02-21T13:35:02.831982Z",
     "shell.execute_reply": "2023-02-21T13:35:02.831123Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取词库\n",
    "with open(MODELS_PATH + 'words_vec.pkl', 'rb') as file:\n",
    "    char2id, id2char = pickle.load(file)\n",
    "\n",
    "# 读取标签内容\n",
    "label_path = sorted([str(p) for p in pathlib.Path(DS_PATH).glob('*.trn')])\n",
    "train_label = list()\n",
    "for path in tqdm(label_path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        # 读取第一行的字标注\n",
    "        train_label.append(file.readline().strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 构建CTC模型的input / output\n",
    "考虑到较大的数据量，可以定义一个generator批量为后续的训练模型输入数据\n",
    "**输入的数据需要按照 tf.keras.backend.ctc_batch_cost 的参数进行构造**\n",
    "\n",
    "y_true：         包含真值标签的张量 (samples, max_string_length)\n",
    "y_pred:          包含预测的张量或 softmax 的输出 (samples, time_steps, num_categories)\n",
    "input_length:    包含预测结果中每个批次序列长度的张量 (samples, 1)\n",
    "label_length:    包含真实标签中每个批次序列长度的张量 (samples, 1)\n",
    "\n",
    "**CTC模型数据构造函数已经包含了数据长度对齐的处理过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.834835Z",
     "iopub.status.busy": "2023-02-21T13:35:02.834560Z",
     "iopub.status.idle": "2023-02-21T13:35:02.844434Z",
     "shell.execute_reply": "2023-02-21T13:35:02.843708Z"
    }
   },
   "outputs": [],
   "source": [
    "def ctc_loss(args):\n",
    "    \"\"\"\n",
    "    构建CTC模型损失函数\n",
    "    :param args: 输入ctc_batch_cost的参数\n",
    "    :return:     (sample, 1) 每个批次内数据包含的CTC损失\n",
    "    \"\"\"\n",
    "    y_true, y_pred, input_length, label_length = args\n",
    "    return ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "def ctc_batch_generator(data, labels, dict_list, n_mfcc, max_length, batch_size):\n",
    "    \"\"\"\n",
    "    构建模型输入使用的CTC模型格式数据，包含长度对齐操作\n",
    "    :param dict_list:\n",
    "    :param data:        音频MFCC特征\n",
    "    :param labels:      语音标注标签(已转换为数字)\n",
    "    :param n_mfcc:      音频MFCC特征维数\n",
    "    :param max_length:  标签最大填充长度\n",
    "    :param batch_size:  每批次送入模型训练的数据数量\n",
    "    :return:            (dict, dict) 包含符合CTC模型格式的input/output数据\n",
    "    \"\"\"\n",
    "    # 初始批次数据量为0\n",
    "    cur_batch = 0\n",
    "    # 生成器\n",
    "    while True:\n",
    "        # 当前批次的数据量\n",
    "        cur_batch += batch_size\n",
    "        \"\"\"\n",
    "        这里使用 offset >= len(data) 判断条件是为了在offset索引超出长度时自动重置该值\n",
    "        防止后续的list操作溢出 (X_data切片取不到batch_size长度的数值)\n",
    "        同时重置offset值为最初的batch_size作为索引，并重新打乱数据\n",
    "        这样可以在之前所有批次数据取完后，重新给模型提供不一样的数据集(从头开始批次生成)\n",
    "        \"\"\"\n",
    "        # 在加载每批次的数据前打乱排序\n",
    "        if cur_batch == batch_size or cur_batch >= len(data):\n",
    "            shuffle_index = np.arange(len(data))\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            # 保证数据与标签一一对应，需要使用同一套已经打乱顺序的标签\n",
    "            data = [data[x] for x in shuffle_index]\n",
    "            labels = [labels[x] for x in shuffle_index]\n",
    "            # 重置cur_batch索引值\n",
    "            cur_batch = batch_size\n",
    "\n",
    "        # 从数据集中获取一个批次的数据，个数为batch_size\n",
    "        X_data = data[cur_batch - batch_size:cur_batch]\n",
    "        y_data = labels[cur_batch - batch_size:cur_batch]\n",
    "\n",
    "        # 获取音频最大帧数作为统一长度 (保证所有数据的完整性)\n",
    "        max_frame = np.max([x.shape[0] for x in X_data])\n",
    "\n",
    "        # 以下过程是先按最大长度创建空间, 然后将没有对齐的数据直接放入空间中, 达到整体对齐的目的（填充法）\n",
    "        X_batch = np.zeros([batch_size, max_frame, n_mfcc])  # 输入的特征长度，填充为最大\n",
    "        y_batch = np.ones([batch_size, max_length]) * len(dict_list)  # 输入的标签长度填充为总词数\n",
    "        X_length = np.zeros([batch_size, 1], dtype=np.int16)  # 输入的数据量=批次数量\n",
    "        y_length = np.zeros([batch_size, 1], dtype=np.int16)  # 输入的特征量=批次数量\n",
    "\n",
    "        # 根据批次数据实时更新CTC输入\n",
    "        for i in range(batch_size):\n",
    "            X_length[i, 0] = X_data[i].shape[0]\n",
    "            X_batch[i, :X_length[i, 0], :] = X_data[i]\n",
    "\n",
    "            y_length[i, 0] = len(y_data[i])\n",
    "            y_batch[i, :y_length[i, 0]] = [dict_list[x] for x in y_data[i]]\n",
    "\n",
    "        # 保存构建的数据结构\n",
    "        ctc_inputs = {'X': X_batch, 'y': y_batch, 'X_length': X_length, 'y_length': y_length}\n",
    "        ctc_output = {'ctc': np.zeros([batch_size])}\n",
    "\n",
    "        # generator 迭代数据\n",
    "        yield ctc_inputs, ctc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构建BiGRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.847023Z",
     "iopub.status.busy": "2023-02-21T13:35:02.846743Z",
     "iopub.status.idle": "2023-02-21T13:35:02.854486Z",
     "shell.execute_reply": "2023-02-21T13:35:02.853711Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_bigru(words_size, n_mfcc, n_cells=512,\n",
    "                n_drop=0.3, max_length=50, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    按照指定参数构建BiGRU模型\n",
    "    :param words_size:     词库大小\n",
    "    :param n_mfcc:         音频MFCC特征维数\n",
    "    :param n_cells:        网络神经元个数\n",
    "    :param n_drop:         GRU层dropout比例数值\n",
    "    :param max_length:     标签长度\n",
    "    :param learning_rate:  ctc_loss优化器学习速率\n",
    "    :return:               (bigru_model, ctc_model) 返回构建的BiGRU模型和CTC Loss模型\n",
    "    \"\"\"\n",
    "    # 定义模型输入数据格式 (输入格式与ctc_batch_generator的返回值一致)\n",
    "    input_data = Input(name='X', shape=(None, n_mfcc))\n",
    "\n",
    "    # 定义BiGRU网络结构 #\n",
    "    # 两层全连接层\n",
    "    dense_1 = Dense(n_cells, activation='relu')(input_data)\n",
    "    dense_2 = Dense(n_cells, activation='relu')(dense_1)\n",
    "    # 两层双向GRU\n",
    "    gru_1 = GRU(n_cells, return_sequences=True, dropout=n_drop)(dense_2)\n",
    "    gru_2 = GRU(n_cells, return_sequences=True, dropout=n_drop, go_backwards=True)(dense_2)\n",
    "    gru_all = add([gru_1, gru_2])  # 合并结构\n",
    "    # 全连接层整合\n",
    "    dense_3 = Dense(n_cells, activation='relu')(gru_all)\n",
    "    # 输出层\n",
    "    dense_output = Dense(words_size + 1, activation='softmax')(dense_3)  # 使用softmax多分类输出\n",
    "    # 保存GRU模型结构\n",
    "    bigru_model = Model(inputs=input_data, outputs=dense_output)\n",
    "\n",
    "    # 定义CTC模型结构 #\n",
    "    # 定义模型输入格式 (y_true, dense_output, input_length, label_length)\n",
    "    y_true = Input(name='y', shape=[max_length], dtype=np.float32)\n",
    "    input_length = Input(name='X_length', shape=[1], dtype=np.int16)\n",
    "    label_length = Input(name='y_length', shape=[1], dtype=np.int16)\n",
    "    # 定义模型输出格式\n",
    "    ctc_loss_out = Lambda(ctc_loss, output_shape=(1,), name='ctc')([y_true, dense_output, input_length, label_length])\n",
    "    # 保存CTC模型结构\n",
    "    ctc_model = Model(inputs=[input_data, y_true, input_length, label_length],\n",
    "                      outputs=ctc_loss_out)\n",
    "\n",
    "    # 定义模型优化器, 编译模型\n",
    "    opt_ada = Adam(learning_rate=learning_rate)\n",
    "    ctc_model.compile(loss={'ctc': lambda y_true, dense_output: dense_output}, optimizer=opt_ada)\n",
    "    # 输出模型信息\n",
    "    ctc_model.summary()\n",
    "\n",
    "    return bigru_model, ctc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 分割数据集 / 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.857567Z",
     "iopub.status.busy": "2023-02-21T13:35:02.857048Z",
     "iopub.status.idle": "2023-02-21T13:35:02.867664Z",
     "shell.execute_reply": "2023-02-21T13:35:02.866800Z"
    }
   },
   "outputs": [],
   "source": [
    "# 每批次数据集大小\n",
    "batch_size = 50\n",
    "# 标签固定长度\n",
    "labels_length = 50\n",
    "# 训练次数\n",
    "epochs = 1000\n",
    "\n",
    "# 划分训练集/测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_ds, train_label,\n",
    "                                                    test_size=0.3, random_state=100)\n",
    "\n",
    "# CTC模型数据generator\n",
    "train_batch = ctc_batch_generator(X_train, y_train, char2id, MFCC_VALUE,\n",
    "                                  batch_size=batch_size, max_length=labels_length)\n",
    "test_batch = ctc_batch_generator(X_test, y_test, char2id, MFCC_VALUE,\n",
    "                                 batch_size=batch_size, max_length=labels_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.870453Z",
     "iopub.status.busy": "2023-02-21T13:35:02.869926Z",
     "iopub.status.idle": "2023-02-21T13:35:04.532821Z",
     "shell.execute_reply": "2023-02-21T13:35:04.532107Z"
    }
   },
   "outputs": [],
   "source": [
    "# 新建模型, ctc_model用于训练, 权重保存在bigru_model中\n",
    "bigru_model, ctc_model = model_bigru(len(char2id), MFCC_VALUE, max_length=labels_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:04.535894Z",
     "iopub.status.busy": "2023-02-21T13:35:04.535343Z",
     "iopub.status.idle": "2023-02-25T00:05:43.777958Z",
     "shell.execute_reply": "2023-02-25T00:05:43.777264Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "history = ctc_model.fit(\n",
    "    train_batch,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_batch,\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    validation_steps=len(X_test) // batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T00:05:43.781052Z",
     "iopub.status.busy": "2023-02-25T00:05:43.780423Z",
     "iopub.status.idle": "2023-02-25T00:05:43.857379Z",
     "shell.execute_reply": "2023-02-25T00:05:43.856680Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "bigru_model.save(MODELS_PATH + 'bigru.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ad05525b964f9a3219eb93b0946337e1e9a632ab3f307ade25b62443cfe7aec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
