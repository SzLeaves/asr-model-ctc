{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CTC + BiGRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:34:59.855505Z",
     "iopub.status.busy": "2023-02-21T13:34:59.855283Z",
     "iopub.status.idle": "2023-02-21T13:35:02.207295Z",
     "shell.execute_reply": "2023-02-21T13:35:02.206567Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 导入keras API\n",
    "keras = tf.keras\n",
    "# 导入模型\n",
    "Model = keras.models.Model\n",
    "Adam = keras.optimizers.Adam\n",
    "ctc_batch_cost = keras.backend.ctc_batch_cost\n",
    "EarlyStopping = keras.callbacks.EarlyStopping\n",
    "Input, Lambda, Add = (\n",
    "    keras.layers.Input,\n",
    "    keras.layers.Lambda,\n",
    "    keras.layers.Add,\n",
    ")\n",
    "Conv1D, GRU, Dense, Dropout = (\n",
    "    keras.layers.Conv1D,\n",
    "    keras.layers.GRU,\n",
    "    keras.layers.Dense,\n",
    "    keras.layers.Dropout,\n",
    ")\n",
    "\n",
    "# 音频/语音标注文件路径\n",
    "DS_PATH = \"../data/\"\n",
    "# 模型文件路径\n",
    "FILES_PATH = \"../output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.210861Z",
     "iopub.status.busy": "2023-02-21T13:35:02.210133Z",
     "iopub.status.idle": "2023-02-21T13:35:02.423958Z",
     "shell.execute_reply": "2023-02-21T13:35:02.422989Z"
    }
   },
   "outputs": [],
   "source": [
    "# 0. 读取数据集\n",
    "\n",
    "# 读取音频特征\n",
    "with open(FILES_PATH + \"dataset/data_mfcc.pkl\", \"rb\") as file:\n",
    "    train_ds, mfcc_mean, mfcc_std = pickle.load(file)\n",
    "\n",
    "# 读取音频标注\n",
    "with open(FILES_PATH + \"dataset/labels.pkl\", \"rb\") as file:\n",
    "    train_label = [x.strip().split() for x in pickle.load(file)]\n",
    "\n",
    "# 读取词库\n",
    "with open(FILES_PATH + \"dataset/words_vec.pkl\", \"rb\") as file:\n",
    "    char2id, id2char = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 构建CTC模型的input / output\n",
    "考虑到较大的数据量，可以定义一个generator批量为后续的训练模型输入数据\n",
    "**输入的数据需要按照 tf.keras.backend.ctc_batch_cost 的参数进行构造**\n",
    "\n",
    "y_true：         包含真值标签的张量 (samples, max_string_length)\n",
    "y_pred:          包含预测的张量或 softmax 的输出 (samples, time_steps, num_categories)\n",
    "input_length:    包含预测结果中每个批次序列长度的张量 (samples, 1)\n",
    "label_length:    包含真实标签中每个批次序列长度的张量 (samples, 1)\n",
    "\n",
    "**CTC模型数据构造函数已经包含了数据长度对齐的处理过程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.834835Z",
     "iopub.status.busy": "2023-02-21T13:35:02.834560Z",
     "iopub.status.idle": "2023-02-21T13:35:02.844434Z",
     "shell.execute_reply": "2023-02-21T13:35:02.843708Z"
    }
   },
   "outputs": [],
   "source": [
    "def ctc_loss(args):\n",
    "    \"\"\"\n",
    "    构建CTC模型损失函数\n",
    "    :param args: 输入ctc_batch_cost的参数\n",
    "    :return:     (sample, 1) 每个批次内数据包含的CTC损失\n",
    "    \"\"\"\n",
    "    y_true, y_pred, input_length, label_length = args\n",
    "    return ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "def ctc_batch_generator(data, labels, dict_list, n_mfcc, max_length, batch_size):\n",
    "    \"\"\"\n",
    "    构建模型输入使用的CTC模型格式数据, 包含长度对齐操作\n",
    "    :param dict_list:\n",
    "    :param data:        音频MFCC特征\n",
    "    :param labels:      语音标注标签(已转换为数字)\n",
    "    :param n_mfcc:      音频MFCC特征维数\n",
    "    :param max_length:  标签最大填充长度\n",
    "    :param batch_size:  每批次送入模型训练的数据数量\n",
    "    :return:            (dict, dict) 包含符合CTC模型格式的input/output数据\n",
    "    \"\"\"\n",
    "    # 初始批次数据量为0\n",
    "    cur_batch = 0\n",
    "    # 生成器\n",
    "    while True:\n",
    "        # 当前批次的数据量\n",
    "        cur_batch += batch_size\n",
    "        \"\"\"\n",
    "        这里使用 offset >= len(data) 判断条件是为了在offset索引超出长度时自动重置该值\n",
    "        防止后续的list操作溢出 (X_data切片取不到batch_size长度的数值)\n",
    "        同时重置offset值为最初的batch_size作为索引, 并重新打乱数据\n",
    "        这样可以在之前所有批次数据取完后，重新给模型提供不一样的数据集(从头开始批次生成)\n",
    "        \"\"\"\n",
    "        # 在加载每批次的数据前打乱排序\n",
    "        if cur_batch == batch_size or cur_batch >= len(data):\n",
    "            shuffle_index = np.arange(len(data))\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            # 保证数据与标签一一对应，需要使用同一套已经打乱顺序的标签\n",
    "            data = [data[x] for x in shuffle_index]\n",
    "            labels = [labels[x] for x in shuffle_index]\n",
    "            # 重置cur_batch索引值\n",
    "            cur_batch = batch_size\n",
    "\n",
    "        # 从数据集中获取一个批次的数据，个数为batch_size\n",
    "        X_data = data[cur_batch - batch_size : cur_batch]\n",
    "        y_data = labels[cur_batch - batch_size : cur_batch]\n",
    "\n",
    "        # 获取音频最大帧数作为统一长度 (保证所有数据的完整性)\n",
    "        max_frame = np.max([x.shape[0] for x in X_data])\n",
    "\n",
    "        # 以下过程是先按最大长度创建空间, 然后将没有对齐的数据直接放入空间中, 达到整体对齐的目的（填充法）\n",
    "        X_batch = np.zeros([batch_size, max_frame, n_mfcc])  # 输入的特征长度，填充为最大\n",
    "        y_batch = np.ones([batch_size, max_length]) * len(dict_list)  # 输入的标签长度填充为总词数\n",
    "        X_length = np.zeros([batch_size, 1], dtype=np.int16)  # 输入的数据量=批次数量\n",
    "        y_length = np.zeros([batch_size, 1], dtype=np.int16)  # 输入的特征量=批次数量\n",
    "\n",
    "        # 根据批次数据实时更新CTC输入\n",
    "        for i in range(batch_size):\n",
    "            X_length[i, 0] = X_data[i].shape[0]\n",
    "            X_batch[i, : X_length[i, 0], :] = X_data[i]\n",
    "\n",
    "            y_length[i, 0] = len(y_data[i])\n",
    "            y_batch[i, : y_length[i, 0]] = [dict_list[x] for x in y_data[i]]\n",
    "\n",
    "        # 保存构建的数据结构\n",
    "        ctc_inputs = {\n",
    "            \"X\": X_batch,\n",
    "            \"y\": y_batch,\n",
    "            \"X_length\": X_length,\n",
    "            \"y_length\": y_length,\n",
    "        }\n",
    "        ctc_output = {\"ctc\": np.zeros([batch_size])}\n",
    "\n",
    "        # generator 迭代数据\n",
    "        yield ctc_inputs, ctc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 构建BiGRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.847023Z",
     "iopub.status.busy": "2023-02-21T13:35:02.846743Z",
     "iopub.status.idle": "2023-02-21T13:35:02.854486Z",
     "shell.execute_reply": "2023-02-21T13:35:02.853711Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_bigru(\n",
    "    words_size, n_mfcc, n_cells=512, n_drop=0.3, max_length=50, learning_rate=0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    按照指定参数构建BiGRU模型\n",
    "    :param words_size:     词库大小\n",
    "    :param n_mfcc:         音频MFCC特征维数\n",
    "    :param n_cells:        网络神经元个数\n",
    "    :param n_drop:         GRU层dropout比例数值\n",
    "    :param max_length:     标签长度\n",
    "    :param learning_rate:  ctc_loss优化器学习速率\n",
    "    :return:               (bigru_model, ctc_model) 返回构建的BiGRU模型和CTC Loss模型\n",
    "    \"\"\"\n",
    "    # 双向GRU单位层数\n",
    "    GRU_NUMS = 3\n",
    "\n",
    "    # 全连接层\n",
    "    def dense(inputs, units, activation, drop, use_bias=True):\n",
    "        inputs = Dropout(drop)(inputs)\n",
    "        return Dense(units=units, activation=activation, use_bias=use_bias)(inputs)\n",
    "\n",
    "    # BiGRU层\n",
    "    def bigru(inputs, drop, units):\n",
    "        inputs = Dropout(drop)(inputs)\n",
    "        gru_1 = GRU(units, return_sequences=True)(inputs)\n",
    "        gru_2 = GRU(units, return_sequences=True, go_backwards=True)(inputs)\n",
    "        return Add()([gru_1, gru_2])\n",
    "\n",
    "    # 定义模型输入数据格式 (输入格式与ctc_batch_generator的返回值一致)\n",
    "    input_data = Input(name=\"X\", shape=(None, n_mfcc))\n",
    "\n",
    "    # 两层全连接\n",
    "    dense_1 = dense(input_data, n_cells, \"relu\", n_drop)\n",
    "    dense_2 = dense(dense_1, n_cells, \"relu\", n_drop)\n",
    "\n",
    "    # 多层双向GRU\n",
    "    gru_all = dense_2\n",
    "    for num_layer in range(GRU_NUMS):\n",
    "        gru_all = bigru(gru_all, n_drop, n_cells)\n",
    "\n",
    "    dense_3 = dense(gru_all, n_cells, \"relu\", n_drop)\n",
    "\n",
    "    # 输出层 使用softmax多分类输出\n",
    "    dense_output = Dense(words_size + 1, activation=\"softmax\")(dense_3)\n",
    "    # 保存GRU模型结构\n",
    "    bigru_model = Model(inputs=input_data, outputs=dense_output)\n",
    "\n",
    "    # 定义CTC模型结构 #\n",
    "    # 定义模型输入格式 (y_true, y_pred, input_length, label_length)\n",
    "    y_true = Input(name=\"y\", shape=[max_length], dtype=np.float32)\n",
    "    input_length = Input(name=\"X_length\", shape=[1], dtype=np.int16)\n",
    "    label_length = Input(name=\"y_length\", shape=[1], dtype=np.int16)\n",
    "\n",
    "    # 定义模型输出格式\n",
    "    ctc_loss_out = Lambda(ctc_loss, output_shape=(1,), name=\"ctc\")(\n",
    "        [y_true, dense_output, input_length, label_length]\n",
    "    )\n",
    "    # 保存CTC模型结构\n",
    "    ctc_model = Model(\n",
    "        inputs=[input_data, y_true, input_length, label_length], outputs=ctc_loss_out\n",
    "    )\n",
    "\n",
    "    # 定义模型优化器, 编译模型\n",
    "\n",
    "    opt_ada = Adam(learning_rate=learning_rate)\n",
    "    ctc_model.compile(\n",
    "        loss={\"ctc\": lambda y_true, dense_output: dense_output}, optimizer=opt_ada\n",
    "    )\n",
    "    # 输出模型信息\n",
    "    ctc_model.summary()\n",
    "\n",
    "    return bigru_model, ctc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 分割数据集 / 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.857567Z",
     "iopub.status.busy": "2023-02-21T13:35:02.857048Z",
     "iopub.status.idle": "2023-02-21T13:35:02.867664Z",
     "shell.execute_reply": "2023-02-21T13:35:02.866800Z"
    }
   },
   "outputs": [],
   "source": [
    "num_mfcc = 32  # mfcc特征维数\n",
    "test_size = 0.2  # 测试集占比\n",
    "labels_length = 50  # 标签固定长度\n",
    "learning_rate = 0.0008  # 学习速率\n",
    "dropout = 0.2  # dropout比例\n",
    "batch_size = 35  # 每批次数据集大小\n",
    "num_cells = 512  # 每层神经元大小\n",
    "epochs = 300  # 训练次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:02.870453Z",
     "iopub.status.busy": "2023-02-21T13:35:02.869926Z",
     "iopub.status.idle": "2023-02-21T13:35:04.532821Z",
     "shell.execute_reply": "2023-02-21T13:35:04.532107Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分训练集/测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_ds, train_label, test_size=test_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-21T13:35:04.535894Z",
     "iopub.status.busy": "2023-02-21T13:35:04.535343Z",
     "iopub.status.idle": "2023-02-25T00:05:43.777958Z",
     "shell.execute_reply": "2023-02-25T00:05:43.777264Z"
    }
   },
   "outputs": [],
   "source": [
    "# CTC模型数据generator\n",
    "train_batch = ctc_batch_generator(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    char2id,\n",
    "    num_mfcc,\n",
    "    batch_size=batch_size,\n",
    "    max_length=labels_length,\n",
    ")\n",
    "\n",
    "test_batch = ctc_batch_generator(\n",
    "    X_test, y_test, char2id, num_mfcc, batch_size=batch_size, max_length=labels_length\n",
    ")\n",
    "\n",
    "# 新建模型, ctc_model用于训练, 权重保存在bigru_model中\n",
    "bigru_model, ctc_model = model_bigru(\n",
    "    len(char2id),\n",
    "    num_mfcc,\n",
    "    n_cells=num_cells,\n",
    "    n_drop=dropout,\n",
    "    max_length=labels_length,\n",
    "    learning_rate=learning_rate,\n",
    ")\n",
    "\n",
    "# 设置回调函数，在训练验证loss没有继续下降时停止训练\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=30, min_delta=1e-5, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 训练模型\n",
    "history = ctc_model.fit(\n",
    "    train_batch,\n",
    "    epochs=epochs,\n",
    "    callbacks=[earlystopping],\n",
    "    validation_data=test_batch,\n",
    "    steps_per_epoch=len(X_train) // batch_size,\n",
    "    validation_steps=len(X_test) // batch_size,\n",
    ")\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"-- Times: %.2fs --\" % end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-25T00:05:43.781052Z",
     "iopub.status.busy": "2023-02-25T00:05:43.780423Z",
     "iopub.status.idle": "2023-02-25T00:05:43.857379Z",
     "shell.execute_reply": "2023-02-25T00:05:43.856680Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "bigru_model.save(FILES_PATH + \"models/bigru.h5\")\n",
    "\n",
    "# 保存训练数据\n",
    "with open(FILES_PATH + \"models/bigru_history.pkl\", \"wb\") as file:\n",
    "    pickle.dump(history, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ad05525b964f9a3219eb93b0946337e1e9a632ab3f307ade25b62443cfe7aec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
